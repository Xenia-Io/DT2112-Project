{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autocoder_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEsKBMh3iPsU",
        "colab_type": "code",
        "outputId": "194d193f-a8a2-446a-be57-69ea71cd95ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import backend as K\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import rmsprop\n",
        "from keras.layers import LeakyReLU\n",
        "import cv2 \n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 32, 32\n",
        "\n",
        "# Process data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# \n",
        "x_train = np.array([cv2.resize(x, (img_rows,img_cols), interpolation=cv2.INTER_CUBIC) for x in x_train])\n",
        "x_test = np.array([cv2.resize(x, (img_rows,img_cols), interpolation=cv2.INTER_CUBIC) for x in x_test])\n",
        "\n",
        "print (x_train.shape)\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Creating model\n",
        "img_shape = (256, 256, 3)\n",
        "batch_size = 3\n",
        "epochs = 5\n",
        "# Generate data parameters\n",
        "samples = 1\n",
        "channel = 3\n",
        "dim = 256\n",
        "\n",
        "train_data = np.array([[[[np.random.randint(low=5, high=20) for x in range(channel)] for y in range(dim)] for z in range(dim)]for n in range(samples)])\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "  encoder = layers.BatchNormalization()(encoder)\n",
        "  encoder = layers.LeakyReLU(alpha=0.3)(encoder)\n",
        "  #encoder = layers.Activation('relu')(encoder)\n",
        "  #encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "  #encoder = layers.BatchNormalization()(encoder)\n",
        "  #encoder = layers.Activation('relu')(encoder)\n",
        "  return encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "  encoder = conv_block(input_tensor, num_filters)\n",
        "  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\n",
        "  return encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "  decoder = layers.BatchNormalization()(decoder)\n",
        "  decoder = layers.LeakyReLU(alpha=0.3)(decoder)\n",
        "  #decoder = layers.Activation('relu')(decoder)\n",
        "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "  decoder = layers.BatchNormalization()(decoder)\n",
        "  decoder = layers.LeakyReLU(alpha=0.3)(decoder)\n",
        "  #decoder = layers.Activation('relu')(decoder)\n",
        "  #decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "  #decoder = layers.BatchNormalization()(decoder)\n",
        "  #decoder = layers.Activation('relu')(decoder)\n",
        "  return decoder\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=input_shape)\n",
        "# 256\n",
        "\n",
        "encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
        "# 128\n",
        "\n",
        "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
        "# 64\n",
        "\n",
        "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
        "# 32\n",
        "\n",
        "#encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
        "# 16\n",
        "\n",
        "#encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
        "# 8\n",
        "\n",
        "center = conv_block(encoder2_pool, 256)\n",
        "# center\n",
        "\n",
        "#decoder4 = decoder_block(center, encoder4, 512)\n",
        "# 16\n",
        "\n",
        "#decoder3 = decoder_block(decoder4, encoder3, 256)\n",
        "# 32\n",
        "\n",
        "decoder2 = decoder_block(center, encoder2, 128)\n",
        "# 64\n",
        "\n",
        "decoder1 = decoder_block(decoder2, encoder1, 64)\n",
        "# 128\n",
        "\n",
        "decoder0 = decoder_block(decoder1, encoder0, 32)\n",
        "# 256\n",
        "\n",
        "outputs = layers.Flatten()(decoder0)\n",
        "\n",
        "outputs = layers.Dense(256, activation='sigmoid')(outputs)\n",
        "\n",
        "outputs = layers.Dense(10, activation='softmax')(outputs)\n",
        "\n",
        "model = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "#model.fit(x_train, y_train,\n",
        "#          batch_size=batch_size,\n",
        "#          epochs=epochs,\n",
        "#          verbose=1,\n",
        "#          validation_data=(x_test, y_test))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 32, 32)\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_36 (InputLayer)           [(None, 32, 32, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 32, 32, 32)   320         input_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 32, 32, 32)   128         conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_218 (LeakyReLU)     (None, 32, 32, 32)   0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_105 (MaxPooling2D (None, 16, 16, 32)   0           leaky_re_lu_218[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 16, 16, 64)   18496       max_pooling2d_105[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 16, 16, 64)   256         conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_219 (LeakyReLU)     (None, 16, 16, 64)   0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_106 (MaxPooling2D (None, 8, 8, 64)     0           leaky_re_lu_219[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 8, 8, 128)    73856       max_pooling2d_106[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 8, 8, 128)    512         conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_220 (LeakyReLU)     (None, 8, 8, 128)    0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_107 (MaxPooling2D (None, 4, 4, 128)    0           leaky_re_lu_220[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 4, 4, 256)    295168      max_pooling2d_107[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 4, 4, 256)    1024        conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_221 (LeakyReLU)     (None, 4, 4, 256)    0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_61 (Conv2DTran (None, 8, 8, 128)    131200      leaky_re_lu_221[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 256)    0           leaky_re_lu_220[0][0]            \n",
            "                                                                 conv2d_transpose_61[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 8, 8, 256)    1024        concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_222 (LeakyReLU)     (None, 8, 8, 256)    0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 8, 8, 128)    295040      leaky_re_lu_222[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 8, 8, 128)    512         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_223 (LeakyReLU)     (None, 8, 8, 128)    0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_62 (Conv2DTran (None, 16, 16, 64)   32832       leaky_re_lu_223[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 16, 16, 128)  0           leaky_re_lu_219[0][0]            \n",
            "                                                                 conv2d_transpose_62[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 16, 16, 128)  512         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_224 (LeakyReLU)     (None, 16, 16, 128)  0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 16, 16, 64)   73792       leaky_re_lu_224[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 16, 16, 64)   256         conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_225 (LeakyReLU)     (None, 16, 16, 64)   0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_63 (Conv2DTran (None, 32, 32, 32)   8224        leaky_re_lu_225[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 32, 32, 64)   0           leaky_re_lu_218[0][0]            \n",
            "                                                                 conv2d_transpose_63[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 32, 32, 64)   256         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_226 (LeakyReLU)     (None, 32, 32, 64)   0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 32, 32, 32)   18464       leaky_re_lu_226[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 32, 32, 32)   128         conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_227 (LeakyReLU)     (None, 32, 32, 32)   0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_13 (Flatten)            (None, 32768)        0           leaky_re_lu_227[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 256)          8388864     flatten_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 10)           2570        dense_26[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 9,343,434\n",
            "Trainable params: 9,341,130\n",
            "Non-trainable params: 2,304\n",
            "__________________________________________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "20076/60000 [=========>....................] - ETA: 58:59 - loss: 0.2108 - acc: 0.9386"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-2c5603f4039b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}